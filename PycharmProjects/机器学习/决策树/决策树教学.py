# #2025.03.23 writen by qibinLuan for version 0

import numpy as np
import math

#一、分类实例及决策树基本概念
#---从实践中总结---
#case 1：
#————————————————————————————————————————————————————————
# 编号   人物     眼睛特征 面部特征 性别  外貌    装备     属性
#  1    哪吒       圆眼    人脸    男   丑      枪      善
#  2   正版哪吒     圆眼    人脸    男   帅      无      善
#  3    鲨鱼哥     小眼   非人脸    男   丑      枪      恶
#  4    申小豹     圆眼    人脸     男  普通     无      善
#  5    结界兽     圆眼   非人脸    无   丑      杖      善
#  6    章鱼哥     小眼   非人脸    男   丑      无      恶
#————————————————————————————————————————————————————————
#小样本情形可人为分类
#分类1：
#                    眼睛特征
#                  小 /   \ 圆
#                   恶    善
#分类2：
#                    面部特征
#                人脸 /    \ 非人脸
#                   善    性别
#                      男 /   \ 无
#                       恶    善
#分类3：
#                     装备
#                无 /  |杖 \ 枪
#                外貌  善  外貌
#        帅，普通 /  \ 丑   丑|
#              善   恶    面部特征
#                   非人脸 /   \ 人脸
#                       恶    善
#特征的层级结构，树状结构，称为决策树，特点：1.属性是纵向层级式，与逻辑回归不同（横向协同式），2.没有误差。
#概念：1.根结点——最上层（特征）层级结点
#     2.叶结点——无下层层级的结点
#     3.非叶结点——有上层及下层的结点
#     4.分支——结点间的连接
#特点：1.叶结点对应分类，非叶结点对应属性（注意，对应属性而不是属性的值。如性别是属性，男或者女是性别的属性值）
#     2.分支对应属性的值
#     3.每个样本都能精确分类，没有误差（过度学习训练姐）
#     4.同一属性可以是不同的非叶结点，但同一属性不会出现在自己属性下的分支的结点上

#分类1比分类2非叶结点少(用到的属性少)，结构简单，泛化能力强。分类2容易过拟合。分类过程即确定层级。
#小样本总结：1.可人为分类；2.可“凭感觉”划定特征的层级，确定决策树结构；3.可构造不同结构的决策树，且不同决策树的结构不同（代表泛化能力）


#case 2：
#————————————————————————————————————————————————————————
# 编号   人物      眼睛特征 面部特征 性别 外貌  装备     属性
#  1    哪吒       圆眼    人脸    男   丑蒙  枪       善
#  2   正版哪吒     圆眼    人脸    男   帅   无        善
#  3   太乙真人     圆眼    人脸    男   丑   拂尘      善
#  4   申公豹       长眼    人脸   男   丑    鞭       善
#  5    李靖       长眼    人脸    男   帅    无       善
#  6   李夫人      长眼    人脸    女   美    无       善
#  7    金吒       长眼    人脸    男   帅    无       善
#  8    木吒       长眼    人脸    男   帅    无       善
#  9    敖丙       圆眼    人脸    男   帅    锤       善
#  10   敖光       长眼    人脸    男   帅    刀       善
#  11  申正道      长眼    人脸    男   普通   无       善
#  12  申小豹      圆眼    人脸    男   普通   无       善
#  13  结界兽      圆眼    非人脸  无    丑    杖       善
#  14  石矶娘娘    小眼    人脸    女   最美   无       善
#  15  土拨鼠      小眼    非人脸  无   普通   无       善
#  16  无极仙翁    小眼    人脸    男    丑   拂尘      恶
#  17  鹿童       长眼    人脸    男    帅    弓       恶
#  18  鹤童       长眼    人脸    女    美    羽毛     恶
#  19  敖顺       长眼    人脸    男    丑    无       恶
#  20  敖润       长眼    人脸    女    美    刺       恶
#  21  敖钦       小眼    非人脸   男   丑    斧       恶
#  22  鲨鱼哥     小眼    非人脸   男   丑    枪       恶
#  23  章鱼哥     小眼    非人脸   男   丑    无       恶
#————————————————————————————————————————————————————————
#样本较多时，1.难以人为分类；2.需要树状结构评价标准（以此标准确定划定层级的方法）

#二、构造-属性划分
#人为偏好：A.奥卡姆剃刀（提高信息纯度）；B.叶结点阈值
#A.奥卡姆剃刀（如非必要，勿增实体）
#- 由以上例子不同的决策树都可以精确分类，但层数越多越复杂，说明对样本学习越精确，
#- 其泛化能力越差。既然不同决策树都可以精确对样本分类，考虑泛化能力，显然结构越
#- 简单越好，即符合奥卡姆剃刀原则。为了使得决策树的层数及分支少，结构简单，显然
#- 在划分的时候，需要尽可能的通过属性的划分使得不同类别区分，即提高划分的区分度。
#- 如将样本的类别看作信息，即需要尽可能地提高信息“纯度”（某属性具体值对应样本中
#- 某一类别出现的次数越多越好。显然信息纯度与某类别在样本中的概率相关。）。
#A.1信息纯度
#-A.1.1 函数形式
#--根据以上分析，可利用信息纯度的指标，构造泛化能力强的决策树，所以需要对信息纯
#--度这一指标量化。假定某一样本的信息纯度f为不同类别信息函数h的和，显然h应为某
#--一类在样本中出现的概率有关。则f的形式为：
#--                     f=∑[i=1,K]{h(Pi)+c}                    （1）
#--                        ∑[i=1,K]{Pi}=1                      （2）
#--其中，i为标签中第i个分类，Pi为第i个分类在样本中的概率，K为标签不同类别的数目
#--（如在case1中，K=2，标签共善、恶两种情况。）。
#--注：∑[i=1,K]{...}表示对{...}求和，求和范围从i=1到i=K。
#--根据拉格朗日乘子法，设拉格朗日乘子为λ，L=f+λ(∑[i=1,K]{Pi}-1)
#--        dL/dPi=0  ——>  (dh/dP)|{P=Pi}+λ=0
#--                  ——>  (dh/dP)|{P=Pi}=-λ=const.，i=1,2,...,K（3）
#--其中，(dh/dP)|{P=Pi}表示h对P求导，求导后取P=Pi。令dh/dP=H(P)，则
#--                          H(Pi)=-λ                          （4）
#--由（4）可知，Pi=H^-1(-λ)时=const.时，f取极值。根据信息纯度的含义，即，当
#--P1=P2=...=PK时，纯度最小。
#--注：1.H^-1为H的反函数，故H是单调的。2.h为非线性函数，否则H为常函数。3.以上
#--对H和h的要求只需在概率的取值区间，即[0,1]上满足即可（实际上f的极值条件还应
#--包括Pi的取值范围，考虑到篇幅及计算复杂度，这里没有考虑。读者可以自行尝试，并
#--由此得到另一个极值条件，即Pi=1，Pj=0，其中j在1到K中取值，且不为i，此时纯度
#--f最大）。
#--由以上讨论可知，对h的形式有一些限制，但也有很大的自由度。以下介绍两种信息纯度
#--的形式，信息熵与基尼指数

#-A.1.2 信息熵&信息增益
#--A.1.2.1 概念
#---信息熵（香农熵），由香农首次提出用以量化信息源（信息的来源，信源可能给出多个
#---事件，因此可视为随机变量）的不确定性。
#---信息量是信息多少的定量描述，表现为消除不确定性的程度‌，对应单个事件的不确定性，
#---显然，信息量与单个事件的概率有关。
#---信息熵是信源各事件信息量的期望（加权平均）
#---设信息量为I，信息熵为S，则，
#---                     S=∑[i=1,N]{Pi*I(Pi)}                    （5）
#---其中，i为信源的第i个事件，N为信源所有的可能事件总数。
#--A.1.2.2 信息熵性质及形式
#---将信源中某两个事件合并为一个事件，则信息熵显然与这种人为的信息拆合无关，设某
#---个信源由N+1个可能事件，且事件对应的概率集合为{P1,P2,...,PN-1,Pa,Pb}，则
#---信息熵为,
#---               S=S_N+1(P1,P2,...,PN-1,Pa,Pb)                 （6）
#---若将最后两个可能的事件人为视为一个子信源的事件，则令PN=Pa+Pb，子信源对应信息
#---量为S_sub(Pa',Pb')，其中Pa'=Pa/PN，Pb'=Pb/PN,此时信息熵S'为，
#---           S'=S_N(P1,P2,...,PN)+PN*S_sub(Pa',Pb')            （7）
#---显然信息熵不应与事件的人为拆合有关，因此S=S'，易得，
#---     S_N+1(P1,P2,...,PN-1,Pa,Pb)=S_N(P1,P2,...,PN)
#---                                 +PN*S_sub(Pa',Pb')          （8）
#---将（5）式代入，得，
#---        Σ[i=1,N-1]{Pi*I(Pi)}+Pa*I(Pa)+Pb*I(Pb)
#---       =Σ[i=1,N]{Pi*I(Pi)}+PN*(Pa'*I(Pa')+Pb'*I(Pb'))        （9）
#---由此可能，
#---   Pa*I(Pa)+Pb*I(Pb)=Pa[I(Pa/PN)+I(PN)]+Pb[I(Pb/PN)+I(PN)]   （10）
#---由Pa与Pb的任意性，不妨取
#---                   I(Pa)=I(Pa/PN)+I(PN)                      （11）
#---令x1=Pa/PN，x2=PN，
#---                   I(x1*x2)=I(x1)+I(x2)                      （12）
#---（12）式两边对x2求导，得
#---            x1*(dI/dx)|{x=x1*x2}=(dI/dx)|{x=x2}              （13）
#---令J(x)=dI/dx，且令x2=1，根据（13）式，得
#---                   x1*J(x1)=J(1)=const.                      （14）
#---由此可知，
#---                      J(x)=c/x                               （15）
#---                    I(x)=c*ln(x)+d                           （16）
#---其中，c，d为常数，将（16）代入（5）式，取c=-1，d=0
#---                S=-∑[i=1,N]{Pi*ln(Pi)}                       （17）
#---注：1.信息量中的常数c，d由信息熵的人为规定确定，假定信息熵是非负的；当某个事件
#---确定发生时，即发生概率为1，其余事件的发生概率为0时，信息熵规定为0。2.实际应用
#---中，通常以2进制数传播信息，故信息熵一般是以2为底的对数，表示为log[2](Pi)，因
#---为log[2](Pi)=log[2](e)*ln(Pi),log[2](Pi)与ln(Pi)=log[e](Pi)，只差一
#---个常数log[2](e)，所以信息熵经常采用以2进制为信息载体的形式，
#---              S=-∑[i=1,N]{Pi*log[2](Pi)}                     （18）
#---信息熵越大，对应信息纯度越小，故可令f=-S，注：S的形式符合前面讨论的f的限制。
#---注：f=-S<0

#-A.1.3 基尼指数
#--如果样本D中的信息纯度高，则从样本中抽取两个样例，属于不同类别的概率小，相反则概
#--率大，因此可以用基尼值Gini衡量信息纯度。
#--            Gini(D)=∑[k=1,K]∑[k'=1,K\{k'=k}]{Pk*Pk'}          （18）
#--其中，[k'=1,K\{k'=k}表示k'从1取到K且k'不等于k。
#--因为∑[k=1,K]∑[k'=k]{Pk*Pk'}=∑[k=1,K]{Pk^2}，所以，
#--      Gini(D)=∑[k=1,K]∑[k'=1,K]{Pk*Pk'}-∑[k=1,K]∑[k'=k]{Pk*Pk'}
#--             =∑[k=1,K]{Pk}∑[k'=1,K]{Pk'}-∑[k=1,K]{Pk^2}
#--             =1-∑[k=1,K]{Pk^2}                                （19）
#--（18）式含义为抽取两次不同类别的样例的概率，（19）式含义为1减抽取的两个样例类别
#--相同的概率。当基尼指数越小，信息纯度越高，因此可令f=-Gini。注：f=-Gini<0


#A.2 划分的区分度与信息纯度
#- 设D是样本（或训练集）的集合，|D|为集合D的势（或基数，即集合中元素的个数，对应样
#- 本数），按某一属性划分样本D时，不同属性值对应的子样本数目|D_v|是不同的。其中v是
#- 某个属性的第v个取值（如case1，若按眼睛特征划分样本，圆眼对应v=1，小眼对应v=2，
#- 圆眼有4个样本，即|D_1|=4；小眼睛|D_2|=2）。此时，划分的区分度F应为不同属性值对
#- 应的子样本D^v的信息纯度f(D^v)的期望（统计加权平均），即
#-              F=C+∑[v=1，V]{|D_v|/|D|*f(D_v)}                   （20）
#- 其中，C为常数。

#- a.取信息熵为度量
#-            F=Gain(D,a)=S(D)-∑[v=1，V_a]{|D_v|/|D|*S(D_v)}      （21）
#- 其中，a表示某个属性（如case1中，令第1个属性为眼睛特征），V_1表示眼睛特征的取值
#- 个数（如case1中，第1个属性的取值个数V_1=2，包含圆眼和小眼）。Gain为信息增益。
#- 决策树的生成对应为优化问题，
#- a*=argmax[a∈A]{Gain(D,a)},其中A为属性集合。

#- b.取基尼值作为度量
#-          F=-Gini_index(D,a)=∑[v=1，V_a]{|D_v|/|D|*Gini(D_v)}   （22）
#- 其中Gini_index为基尼指数。决策树的生成对应为优化问题，
#- a*=argmin[a∈A]{Gini_index(D,a)},其中A为属性集合。                （23）

#A.3 决策树实例
#- 考虑采用增加信息增益的标准生成决策树
#- case2的样本D中，共23个样例，其中善15个，恶8个。

def comS(karr,T):
#comS为计算样本的信息熵函数，形参中karr为各类别出现次数的列表，T为样例总数
#如一个样本中若出现0类的次数为a0，1类为a1，2类为a2，则karr=[a1,a2,a3],T=a1+a2+a3
#参数T允许小于等于0，样例总数通过计算得到。
#- 对应信息熵,
    L=len(karr)
    Sum=sum(karr)
    if T>0:
        if Sum!=T:
            print("error:check input information of function of comS")
            exit()
    if T<=0:
        T=Sum

    S=0
    for i in range(L):
        if karr[i]==0:
            continue
        pi=karr[i]/T
        S=S-pi*math.log2(pi)
    return S
#现在用这个函数举一个例子
D=23
D_arr=[15,8]
S=comS(D_arr,D)
print("case2中根结点信息熵为",S)
#- S(D)=15/23*log[2]{15/23}+8/23*log[2]{8/23}=0.9321115676166747
#
#现在计算某一个分类的熵（先举例子）
#- a.若以眼睛特征为根结点，则圆眼睛样例共6个，其中善为6，恶为0；
#- 小眼睛样例共6个，其中善为2个，恶为4个；
#- 长眼睛样例共11个，其中善为7个，恶为4个；
D1char='根结点（第一层）：眼睛特征'
D1_1_char=['圆眼睛','小眼睛','长眼睛']
#- 圆眼睛取值对应的子样本信息，
D1_1_1=6
D1_1_1_arr=[6,0]
#S1_1_1=comS(D1_1_1_arr,D1_1_1)
#print("case2中眼睛特征作为根结点，圆眼睛的信息量加权",S1_1_1)
#- 小眼睛取值对应的子样本信息，
D1_1_2=6
D1_1_2_arr=[2,4]
#S1_1_2=comS(D1_1_2_arr,D1_1_2)
#print("case2中眼睛特征作为根结点，小眼睛的信息量加权",S1_1_2)
#- 长眼睛取值对应的子样本信息，
D1_1_3=11
D1_1_3_arr=[7,4]
#S1_1_3=comS(D1_1_3_arr,D1_1_3)
#print("case2中眼睛特征作为根结点，长眼睛的信息量加权",S1_1_3)
#D1_1_arr=[[D1_1_1_arr,D1_1_1],[D1_1_2_arr,D1_1_2],[D1_1_3_arr,D1_1_3]]
D1_1_arr=[D1_1_1_arr,D1_1_2_arr,D1_1_3_arr]

def comSD(floorchar,Dfig_char,Darr,D,diaflag):
#comSD计算按某一属性划分的信息增益
#D为样本中样例总数，若D<=0,D通过Darr中的数据计算
#floorchar为属性名字的字符串，Dfig_char为属性具体取值的字符串，Darr为不同属性取值
#不同类别个数的列表，如某N个样例的样本有两个类别，某属性有两个取值，其中一个取值对应的
#两个类别数目分别为m1和N-m1，另一个取值为m2和N-m2，则Darr=[[m1,N-m1],[m2,N-m2]]。
#D为当前样本总数。diaflag为输出
#对话的控制参数。
    #属性个数取值的数据校验
    l1=len(Dfig_char)#属性不同取值个数
    l1p=len(Darr)#属性不同取值个数
    if l1!=l1p:
        print("error1:check the second and third parameters of function of comSD")
        exit()

    ks=len(Darr[0])#第1个属性取值的数据中标签（种类）个数
    nums=0#从Darr中计算的总样本中样例的数目
    Dv=[]#列表，存放不同属性取值的子样本中样例的数目
    for i in range(l1):#对不同属性取值循环
        #标签（种类）个数的数据校验
        if i !=0 :
           k=len(Darr[i])#除第1个属性取值的其他取值数据中标签（种类）个数
           if k != ks:
               print("error2:check the third parameter of function of comSD")
               exit()
        tempI=sum(Darr[i])#不同属性取值的子样本中样例的数目
        Dv.append(tempI)
        nums=nums+tempI
    if D > 0:
        #样本中样例数目的数据校验
        if nums!=D:
            print("error3:check the third and forth parameters of function of comSD")
            exit()
    if D <= 0:
        D=nums

    Splus=0#信息增益
    for i in range(l1):#对不同属性取值循环
#        print('i-value',i)
        Stemp=comS(Darr[i],0) #计算不同属性的信息熵
        if diaflag==2:
            print(floorchar + ':', Dfig_char[i], "信息量:", Stemp)
        Stempw=(Dv[i]/D)*Stemp   #信息熵的加权
        if diaflag>=1:
            print(floorchar+':',Dfig_char[i],"信息量加权:", Stempw)
        Splus=Splus+Stempw
    return Splus
#现在算另一个特征的信息熵
SDiv=comSD(D1char,D1_1_char,D1_1_arr,D,2)
SD=S-SDiv #信息增益
print(D1char,'划分后信息熵增益为：',SD)
#- 若以眼睛特征为根结点，则划分后信息增益为0.24028381305432778

#- b.若以面部特征为根结点，则人脸样例共18个，其中善为13，恶为5；
#- 非人脸样例共5个，其中善为2个，恶为3个；
D1char='根结点（第一层）：面部特征'
D1_2_char=['人脸','非人脸']
#- 人脸取值对应的子样本信息，
D1_2_1=18
D1_2_1_arr=[13,5]
#- 非人脸取值对应的子样本信息，
D1_2_2=5
D1_2_2_arr=[2,3]
D1_2_arr=[D1_2_1_arr,D1_2_2_arr]

SDiv=comSD(D1char,D1_2_char,D1_2_arr,D,2)
SD=S-SDiv
print(D1char,'划分后信息熵增益为：',SD)
#- 若以面部特征为根结点，则划分后信息增益为0.05393564640085036
#- 由此可见，以眼睛特征作为当前划分比面部特征好。
#- 当属性较多时，需要自动扫描不同属性，计算信息增益。因此需要对训
#- 练集格式化，以方便编写决策树生成的自动化脚本。

#case3
#     --------------- 属性 --------------    标签
#编号 色泽   根蒂   敲声   纹理   脐部   触感    好瓜
# 1  青绿1  蜷缩1  浊响1  清晰1  凹陷1  硬滑1    是1
# 2  乌黑2    1   沉闷2    1     1      1      1
# 3    2     1     1      1     1      1      1
# 4    1     1     2     1     1      1       1
# 5    3     1     1     1     1      1       1
# 6    1   稍蜷2    1     1    稍凹2  软粘2     1
# 7    2     2     1    稍糊2   2      2       1
# 8    2     2     1     1     2      1       1
# 9    2     2     2     2     2      1       0
#10    1   硬挺3  清脆3    1    平坦3   2       0
#11    3     3     3     3     3      1       0
#12    3     1     1     3     3      2       0
#13    1     2     1     2     1      1       0
#14  浅白3    2     2     2     1      1       0
#15    2     2     1     1     2      2       0
#16    3     1     1    模糊3   3      1       0
#17    1     1     2      2    2      1       0

#- 训练集格式化
#- 属性字符串
fnc=['色泽','根蒂','敲声','纹理','脐部','触感']
#- 标签字符串
tnc=['好','坏']
#- 属性取值字符串
fnvc1=['青绿','乌黑','浅白']
fnvc2=['蜷缩','稍蜷','硬挺']
fnvc3=['浊响','沉闷','清脆']
fnvc4=['清晰','稍糊','模糊']
fnvc5=['凹陷','稍凹','平坦']
fnvc6=['硬滑','软粘']
fnvc=[fnvc1,fnvc2,fnvc3,fnvc4,fnvc5,fnvc6]
#- 数据集
#- 属性取值按索引号数值化，注意此例是从1开始。如第一个属性对应色泽，数字3对应浅白
#- 标签类别按索引号数值化，注意此例是从0开始。
data=[[1,1,1,1,1,1,0],[2,1,2,1,1,1,0],[2,1,1,1,1,1,0],
      [1,1,2,1,1,1,0],[5,3,1,1,1,1,0],[1,2,1,1,2,2,0],
      [2,2,1,2,2,2,0],[2,2,1,1,2,1,0],[2,2,2,2,2,1,1],
      [1,3,3,1,3,2,1],[3,3,3,3,3,1,1],[3,1,1,3,3,2,1],
      [1,2,1,2,1,1,1],[3,2,2,2,1,1,1],[2,2,1,1,2,2,1],
      [3,1,1,3,3,1,1],[1,1,2,2,2,1,1]]
fign=len(fnc)#属性个数
datan=len(data)#数据集样例个数
kn=len(tnc)#类别个数
localflow='根结点'
Gaina=np.zeros(fign)#
tempchar0='=========='
tempcharx='**********'
tempcharsp='  '

for i in range(fign): #扫描不同属性
    tempchar1='按'+fnc[i]+'划分'
    inforchar=tempchar0+tempchar1+tempchar0
    print(inforchar)

    #构造每个属性的分类情况列表，以用函数comSD计算信息增益
    D1_X_arr=[]
    figvn=len(fnvc[i])#第i个属性的取值个数
    #每个取值的各个例子数目
    tempsub=np.zeros((figvn,kn),dtype=int)#某属性不同属性值的分类数据
    #print(tempsub)
    for j in range(datan):#扫描不同样例
        for k in range(figvn):#扫描不同属性值
            kp=k+1#数据中不同属性取值从1开始排序
            if data[j][i]==kp:
                for l in range(kn):#扫描不同种类
#                   if data[j][fign]==l:
                    if data[j][-1] == l:
                        tempsub[k][l]=tempsub[k][l]+1
    tempcsd=comSD(localflow, fnvc[i], tempsub, 0,1)
    #计算样本未划分前的信息熵
    if i == 1:
        Dsample=np.zeros(kn,dtype=int)
        #print(Dsample)
        for l in range(kn):
            for k in range(figvn):
                Dsample[l]=Dsample[l]+tempsub[k][l]
        S = comS(Dsample, 0)
    Gaina[i] = S - tempcsd
    print(tempchar1, '划分后信息熵增益为：', Gaina[i])
tempI=np.argmax(Gaina) #取信息增益最大值对应的序号
optfig=fnc[tempI]
tempchar=tempchar0*2+tempcharsp+tempcharx+tempcharsp+tempchar0*2
print(tempchar)
print('当前最优划分属性：',optfig)
print('划分后信息熵增益为：', Gaina[tempI])
print(tempchar)














